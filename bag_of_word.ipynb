{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#读取工作簿和工作簿中的工作表\n",
    "data_frame=pd.read_excel('Problem_C_Data_Wordle.xlsx')\n",
    "# print(data_frame)\n",
    "data = list(reversed(data_frame.loc[ 1: ,\"Unnamed: 3\"].tolist()))\n",
    "data_1=list(reversed(data_frame.loc[ : ,\"Percent in \"]))[:-1]\n",
    "data_2=list(reversed(data_frame.loc[ 1: ,\"Unnamed: 7\"].tolist()))\n",
    "data_3=list(reversed(data_frame.loc[ 1: ,\"Unnamed: 8\"].tolist()))\n",
    "data_4=list(reversed(data_frame.loc[ 1: ,\"Unnamed: 9\"].tolist()))\n",
    "data_5=list(reversed(data_frame.loc[ 1: ,\"Unnamed: 10\"].tolist()))\n",
    "data_6=list(reversed(data_frame.loc[ 1: ,\"Unnamed: 11\"].tolist()))\n",
    "data_7=list(reversed(data_frame.loc[ 1: ,\"Unnamed: 12\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]\n",
      "[3, 5, 3, 4, 9, 4, 2, 4, 9, 9, 8, 2, 16, 8, 8, 3, 5, 1, 6, 4, 9, 4, 7, 2, 10, 13, 13, 7, 10, 4, 3, 3, 10, 5, 8, 4, 7, 6, 6, 6, 4, 6, 3, 1, 4, 9, 14, 5, 6, 2, 5, 9, 8, 2, 7, 8, 5, 9, 8, 9, 5, 5, 8, 6, 7, 4, 5, 16, 7, 5, 8, 5, 4, 14, 2, 4, 14, 5, 2, 2, 6, 3, 5, 2, 4, 16, 2, 3, 2, 2, 2, 12, 21, 10, 3, 5, 4, 6, 11, 3, 4, 8, 2, 5, 2, 19, 6, 7, 3, 13, 6, 2, 19, 2, 1, 10, 8, 26, 3, 4, 3, 2, 14, 2, 9, 2, 6, 10, 4, 2, 4, 8, 4, 4, 9, 7, 5, 5, 2, 7, 8, 4, 6, 6, 6, 5, 2, 13, 5, 6, 2, 3, 6, 6, 4, 2, 12, 3, 16, 3, 7, 6, 1, 10, 5, 8, 5, 4, 6, 3, 9, 6, 2, 1, 1, 5, 3, 2, 2, 6, 0, 2, 5, 6, 7, 3, 8, 7, 4, 2, 2, 3, 4, 5, 4, 6, 3, 2, 6, 4, 2, 1, 7, 5, 2, 8, 5, 0, 5, 4, 1, 17, 2, 3, 2, 4, 6, 4, 1, 2, 4, 3, 6, 3, 4, 22, 7, 7, 2, 2, 8, 6, 2, 2, 6, 4, 5, 2, 12, 1, 6, 1, 4, 3, 4, 8, 4, 1, 1, 3, 5, 12, 0, 11, 9, 5, 6, 5, 14, 6, 14, 10, 5, 2, 3, 4, 8, 4, 6, 10, 3, 9, 10, 2, 2, 2, 3, 4, 2, 5, 3, 7, 8, 12, 5, 3, 5, 4, 7, 1, 7, 2, 6, 4, 7, 3, 2, 3, 1, 6, 18, 5, 14, 19, 6, 4, 16, 11, 5, 4, 8, 6, 5, 5, 14, 2, 7, 6, 5, 10, 12, 13, 8, 17, 6, 10, 3, 6, 2, 6, 4, 17, 2, 6, 6, 3, 10, 3, 7, 5, 6, 3, 7, 6, 7, 8, 14, 10, 5, 1, 7, 11, 5, 2, 2, 3, 2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "print(data_1)\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data)\n",
    "letter2idx={}\n",
    "train_X=[]\n",
    "train_Y=[]\n",
    "cnt=0\n",
    "for item in data:\n",
    "    letters=list(item)\n",
    "    x=[0 for i in range(26)]\n",
    "    y=[data_1[cnt]/100,data_2[cnt]/100,data_3[cnt]/100,data_4[cnt]/100,data_5[cnt]/100,data_6[cnt]/100,data_7[cnt]/100]\n",
    "    cnt+=1\n",
    "    for letter in letters:\n",
    "        if(letter==\" \"):\n",
    "            continue\n",
    "        if(letter not in letter2idx.keys()):\n",
    "            letter2idx[letter]=len(letter2idx)\n",
    "            #print(letter,letters)\n",
    "        #print(letter2idx[letter])\n",
    "        x[letter2idx[letter]]+=1\n",
    "    train_X.append(x)\n",
    "    train_Y.append(y)\n",
    "#print(train_X,letter2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # self.embedding = nn.Embedding(26, 64)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.fc1 = nn.Linear(26, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu=nn.ReLU(True)\n",
    "        self.fc3 = nn.Linear(128, 7)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "    \t#x:[batchsize, max_length]\n",
    "        #out = self.embedding(x)# [batchsize, max_length, embedding_size]\n",
    "        #这里相当于增加了一个in_channels维度，TextCNN输入的通道维数为1\n",
    "        #out = out.unsqueeze(1)# [batchsize, 1, max_length, embedding_size]\n",
    "        # 这里分别使用不同size的卷积核进行卷积，以(2,, 300)为例\n",
    "        # 输入：[64, 1, 32, 300]\n",
    "        # 进行卷积得到输出：[64, 128, 32-2+1=31, 1]\n",
    "        # 经过卷积之后经过一个relu，然后变形成[64, 128, 31]\n",
    "        # 然后经过1维最大池化得[64, 128, 1]，再变形为[64, 128]\n",
    "        # 最终将不同卷积核卷积后的结果拼接为,[64, 128*3]\n",
    "        out=self.fc1(x)\n",
    "        out=self.fc2(out)\n",
    "        out=self.relu(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        pred=self.softmax(out)\n",
    "        return out,pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "model=TextCNN()\n",
    "loss_func=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-2)\n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0, Loss: 0.00478\n",
      "Epoch:    1, Loss: 0.00299\n",
      "Epoch:    2, Loss: 0.00247\n",
      "Epoch:    3, Loss: 0.00223\n",
      "Epoch:    4, Loss: 0.00220\n",
      "Epoch:    5, Loss: 0.00194\n",
      "Epoch:    6, Loss: 0.00203\n",
      "Epoch:    7, Loss: 0.00202\n",
      "Epoch:    8, Loss: 0.00173\n",
      "Epoch:    9, Loss: 0.00133\n",
      "Epoch:   10, Loss: 0.00128\n",
      "Epoch:   11, Loss: 0.00118\n",
      "Epoch:   12, Loss: 0.00119\n",
      "Epoch:   13, Loss: 0.00126\n",
      "Epoch:   14, Loss: 0.00114\n",
      "Epoch:   15, Loss: 0.00113\n",
      "Epoch:   16, Loss: 0.00123\n",
      "Epoch:   17, Loss: 0.00117\n",
      "Epoch:   18, Loss: 0.00127\n",
      "Epoch:   19, Loss: 0.00140\n",
      "Epoch:   20, Loss: 0.00122\n",
      "Epoch:   21, Loss: 0.00128\n",
      "Epoch:   22, Loss: 0.00108\n",
      "Epoch:   23, Loss: 0.00081\n",
      "Epoch:   24, Loss: 0.00083\n",
      "Epoch:   25, Loss: 0.00084\n",
      "Epoch:   26, Loss: 0.00113\n",
      "Epoch:   27, Loss: 0.00139\n",
      "Epoch:   28, Loss: 0.00159\n",
      "Epoch:   29, Loss: 0.00171\n",
      "Epoch:   30, Loss: 0.00153\n",
      "Epoch:   31, Loss: 0.00159\n",
      "Epoch:   32, Loss: 0.00170\n",
      "Epoch:   33, Loss: 0.00158\n",
      "Epoch:   34, Loss: 0.00121\n",
      "Epoch:   35, Loss: 0.00117\n",
      "Epoch:   36, Loss: 0.00106\n",
      "Epoch:   37, Loss: 0.00108\n",
      "Epoch:   38, Loss: 0.00136\n",
      "Epoch:   39, Loss: 0.00116\n",
      "Epoch:   40, Loss: 0.00102\n",
      "Epoch:   41, Loss: 0.00102\n",
      "Epoch:   42, Loss: 0.00119\n",
      "Epoch:   43, Loss: 0.00136\n",
      "Epoch:   44, Loss: 0.00144\n",
      "Epoch:   45, Loss: 0.00126\n",
      "Epoch:   46, Loss: 0.00141\n",
      "Epoch:   47, Loss: 0.00129\n",
      "Epoch:   48, Loss: 0.00133\n",
      "Epoch:   49, Loss: 0.00124\n",
      "Epoch:   50, Loss: 0.00106\n",
      "Epoch:   51, Loss: 0.00093\n",
      "Epoch:   52, Loss: 0.00094\n",
      "Epoch:   53, Loss: 0.00101\n",
      "Epoch:   54, Loss: 0.00094\n",
      "Epoch:   55, Loss: 0.00100\n",
      "Epoch:   56, Loss: 0.00113\n",
      "Epoch:   57, Loss: 0.00105\n",
      "Epoch:   58, Loss: 0.00111\n",
      "Epoch:   59, Loss: 0.00101\n",
      "Epoch:   60, Loss: 0.00113\n",
      "Epoch:   61, Loss: 0.00111\n",
      "Epoch:   62, Loss: 0.00100\n",
      "Epoch:   63, Loss: 0.00098\n",
      "Epoch:   64, Loss: 0.00083\n",
      "Epoch:   65, Loss: 0.00080\n",
      "Epoch:   66, Loss: 0.00078\n",
      "Epoch:   67, Loss: 0.00071\n",
      "Epoch:   68, Loss: 0.00077\n",
      "Epoch:   69, Loss: 0.00092\n",
      "Epoch:   70, Loss: 0.00094\n",
      "Epoch:   71, Loss: 0.00094\n",
      "Epoch:   72, Loss: 0.00095\n",
      "Epoch:   73, Loss: 0.00089\n",
      "Epoch:   74, Loss: 0.00077\n",
      "Epoch:   75, Loss: 0.00075\n",
      "Epoch:   76, Loss: 0.00068\n",
      "Epoch:   77, Loss: 0.00075\n",
      "Epoch:   78, Loss: 0.00083\n",
      "Epoch:   79, Loss: 0.00064\n",
      "Epoch:   80, Loss: 0.00068\n",
      "Epoch:   81, Loss: 0.00063\n",
      "Epoch:   82, Loss: 0.00075\n",
      "Epoch:   83, Loss: 0.00073\n",
      "Epoch:   84, Loss: 0.00074\n",
      "Epoch:   85, Loss: 0.00080\n",
      "Epoch:   86, Loss: 0.00091\n",
      "Epoch:   87, Loss: 0.00102\n",
      "Epoch:   88, Loss: 0.00085\n",
      "Epoch:   89, Loss: 0.00099\n",
      "Epoch:   90, Loss: 0.00086\n",
      "Epoch:   91, Loss: 0.00082\n",
      "Epoch:   92, Loss: 0.00089\n",
      "Epoch:   93, Loss: 0.00085\n",
      "Epoch:   94, Loss: 0.00082\n",
      "Epoch:   95, Loss: 0.00073\n",
      "Epoch:   96, Loss: 0.00075\n",
      "Epoch:   97, Loss: 0.00071\n",
      "Epoch:   98, Loss: 0.00067\n",
      "Epoch:   99, Loss: 0.00058\n",
      "Epoch:  100, Loss: 0.00059\n",
      "Epoch:  101, Loss: 0.00057\n",
      "Epoch:  102, Loss: 0.00081\n",
      "Epoch:  103, Loss: 0.00080\n",
      "Epoch:  104, Loss: 0.00070\n",
      "Epoch:  105, Loss: 0.00065\n",
      "Epoch:  106, Loss: 0.00056\n",
      "Epoch:  107, Loss: 0.00062\n",
      "Epoch:  108, Loss: 0.00066\n",
      "Epoch:  109, Loss: 0.00064\n",
      "Epoch:  110, Loss: 0.00070\n",
      "Epoch:  111, Loss: 0.00076\n",
      "Epoch:  112, Loss: 0.00080\n",
      "Epoch:  113, Loss: 0.00085\n",
      "Epoch:  114, Loss: 0.00075\n",
      "Epoch:  115, Loss: 0.00086\n",
      "Epoch:  116, Loss: 0.00073\n",
      "Epoch:  117, Loss: 0.00072\n",
      "Epoch:  118, Loss: 0.00061\n",
      "Epoch:  119, Loss: 0.00055\n",
      "Epoch:  120, Loss: 0.00059\n",
      "Epoch:  121, Loss: 0.00072\n",
      "Epoch:  122, Loss: 0.00070\n",
      "Epoch:  123, Loss: 0.00073\n",
      "Epoch:  124, Loss: 0.00064\n",
      "Epoch:  125, Loss: 0.00070\n",
      "Epoch:  126, Loss: 0.00092\n",
      "Epoch:  127, Loss: 0.00092\n",
      "Epoch:  128, Loss: 0.00103\n",
      "Epoch:  129, Loss: 0.00090\n",
      "Epoch:  130, Loss: 0.00083\n",
      "Epoch:  131, Loss: 0.00076\n",
      "Epoch:  132, Loss: 0.00069\n",
      "Epoch:  133, Loss: 0.00058\n",
      "Epoch:  134, Loss: 0.00054\n",
      "Epoch:  135, Loss: 0.00048\n",
      "Epoch:  136, Loss: 0.00048\n",
      "Epoch:  137, Loss: 0.00049\n",
      "Epoch:  138, Loss: 0.00061\n",
      "Epoch:  139, Loss: 0.00058\n",
      "Epoch:  140, Loss: 0.00058\n",
      "Epoch:  141, Loss: 0.00055\n",
      "Epoch:  142, Loss: 0.00058\n",
      "Epoch:  143, Loss: 0.00061\n",
      "Epoch:  144, Loss: 0.00052\n",
      "Epoch:  145, Loss: 0.00051\n",
      "Epoch:  146, Loss: 0.00059\n",
      "Epoch:  147, Loss: 0.00058\n",
      "Epoch:  148, Loss: 0.00060\n",
      "Epoch:  149, Loss: 0.00050\n",
      "Epoch:  150, Loss: 0.00053\n",
      "Epoch:  151, Loss: 0.00041\n",
      "Epoch:  152, Loss: 0.00040\n",
      "Epoch:  153, Loss: 0.00049\n",
      "Epoch:  154, Loss: 0.00059\n",
      "Epoch:  155, Loss: 0.00057\n",
      "Epoch:  156, Loss: 0.00069\n",
      "Epoch:  157, Loss: 0.00097\n",
      "Epoch:  158, Loss: 0.00101\n",
      "Epoch:  159, Loss: 0.00077\n",
      "Epoch:  160, Loss: 0.00073\n",
      "Epoch:  161, Loss: 0.00061\n",
      "Epoch:  162, Loss: 0.00057\n",
      "Epoch:  163, Loss: 0.00051\n",
      "Epoch:  164, Loss: 0.00074\n",
      "Epoch:  165, Loss: 0.00071\n",
      "Epoch:  166, Loss: 0.00082\n",
      "Epoch:  167, Loss: 0.00088\n",
      "Epoch:  168, Loss: 0.00097\n",
      "Epoch:  169, Loss: 0.00090\n",
      "Epoch:  170, Loss: 0.00063\n",
      "Epoch:  171, Loss: 0.00061\n",
      "Epoch:  172, Loss: 0.00052\n",
      "Epoch:  173, Loss: 0.00054\n",
      "Epoch:  174, Loss: 0.00049\n",
      "Epoch:  175, Loss: 0.00048\n",
      "Epoch:  176, Loss: 0.00046\n",
      "Epoch:  177, Loss: 0.00045\n",
      "Epoch:  178, Loss: 0.00051\n",
      "Epoch:  179, Loss: 0.00050\n",
      "Epoch:  180, Loss: 0.00051\n",
      "Epoch:  181, Loss: 0.00069\n",
      "Epoch:  182, Loss: 0.00073\n",
      "Epoch:  183, Loss: 0.00069\n",
      "Epoch:  184, Loss: 0.00081\n",
      "Epoch:  185, Loss: 0.00080\n",
      "Epoch:  186, Loss: 0.00066\n",
      "Epoch:  187, Loss: 0.00063\n",
      "Epoch:  188, Loss: 0.00060\n",
      "Epoch:  189, Loss: 0.00075\n",
      "Epoch:  190, Loss: 0.00092\n",
      "Epoch:  191, Loss: 0.00088\n",
      "Epoch:  192, Loss: 0.00102\n",
      "Epoch:  193, Loss: 0.00091\n",
      "Epoch:  194, Loss: 0.00093\n",
      "Epoch:  195, Loss: 0.00081\n",
      "Epoch:  196, Loss: 0.00084\n",
      "Epoch:  197, Loss: 0.00080\n",
      "Epoch:  198, Loss: 0.00075\n",
      "Epoch:  199, Loss: 0.00093\n",
      "Epoch:  200, Loss: 0.00083\n",
      "Epoch:  201, Loss: 0.00094\n",
      "Epoch:  202, Loss: 0.00083\n",
      "Epoch:  203, Loss: 0.00092\n",
      "Epoch:  204, Loss: 0.00081\n",
      "Epoch:  205, Loss: 0.00085\n",
      "Epoch:  206, Loss: 0.00076\n",
      "Epoch:  207, Loss: 0.00069\n",
      "Epoch:  208, Loss: 0.00065\n",
      "Epoch:  209, Loss: 0.00071\n",
      "Epoch:  210, Loss: 0.00065\n",
      "Epoch:  211, Loss: 0.00058\n",
      "Epoch:  212, Loss: 0.00063\n",
      "Epoch:  213, Loss: 0.00066\n",
      "Epoch:  214, Loss: 0.00061\n",
      "Epoch:  215, Loss: 0.00053\n",
      "Epoch:  216, Loss: 0.00049\n",
      "Epoch:  217, Loss: 0.00049\n",
      "Epoch:  218, Loss: 0.00048\n",
      "Epoch:  219, Loss: 0.00053\n",
      "Epoch:  220, Loss: 0.00049\n",
      "Epoch:  221, Loss: 0.00045\n",
      "Epoch:  222, Loss: 0.00049\n",
      "Epoch:  223, Loss: 0.00067\n",
      "Epoch:  224, Loss: 0.00060\n",
      "Epoch:  225, Loss: 0.00066\n",
      "Epoch:  226, Loss: 0.00074\n",
      "Epoch:  227, Loss: 0.00089\n",
      "Epoch:  228, Loss: 0.00092\n",
      "Epoch:  229, Loss: 0.00085\n",
      "Epoch:  230, Loss: 0.00090\n",
      "Epoch:  231, Loss: 0.00079\n",
      "Epoch:  232, Loss: 0.00066\n",
      "Epoch:  233, Loss: 0.00062\n",
      "Epoch:  234, Loss: 0.00059\n",
      "Epoch:  235, Loss: 0.00071\n",
      "Epoch:  236, Loss: 0.00068\n",
      "Epoch:  237, Loss: 0.00067\n",
      "Epoch:  238, Loss: 0.00088\n",
      "Epoch:  239, Loss: 0.00071\n",
      "Epoch:  240, Loss: 0.00062\n",
      "Epoch:  241, Loss: 0.00071\n",
      "Epoch:  242, Loss: 0.00072\n",
      "Epoch:  243, Loss: 0.00067\n",
      "Epoch:  244, Loss: 0.00061\n",
      "Epoch:  245, Loss: 0.00066\n",
      "Epoch:  246, Loss: 0.00067\n",
      "Epoch:  247, Loss: 0.00063\n",
      "Epoch:  248, Loss: 0.00069\n",
      "Epoch:  249, Loss: 0.00070\n",
      "Epoch:  250, Loss: 0.00072\n",
      "Epoch:  251, Loss: 0.00075\n",
      "Epoch:  252, Loss: 0.00073\n",
      "Epoch:  253, Loss: 0.00072\n",
      "Epoch:  254, Loss: 0.00068\n",
      "Epoch:  255, Loss: 0.00067\n",
      "Epoch:  256, Loss: 0.00068\n",
      "Epoch:  257, Loss: 0.00070\n",
      "Epoch:  258, Loss: 0.00064\n",
      "Epoch:  259, Loss: 0.00060\n",
      "Epoch:  260, Loss: 0.00069\n",
      "Epoch:  261, Loss: 0.00058\n",
      "Epoch:  262, Loss: 0.00062\n",
      "Epoch:  263, Loss: 0.00077\n",
      "Epoch:  264, Loss: 0.00083\n",
      "Epoch:  265, Loss: 0.00086\n",
      "Epoch:  266, Loss: 0.00075\n",
      "Epoch:  267, Loss: 0.00071\n",
      "Epoch:  268, Loss: 0.00064\n",
      "Epoch:  269, Loss: 0.00061\n",
      "Epoch:  270, Loss: 0.00057\n",
      "Epoch:  271, Loss: 0.00062\n",
      "Epoch:  272, Loss: 0.00071\n",
      "Epoch:  273, Loss: 0.00068\n",
      "Epoch:  274, Loss: 0.00065\n",
      "Epoch:  275, Loss: 0.00060\n",
      "Epoch:  276, Loss: 0.00060\n",
      "Epoch:  277, Loss: 0.00071\n",
      "Epoch:  278, Loss: 0.00060\n",
      "Epoch:  279, Loss: 0.00051\n",
      "Epoch:  280, Loss: 0.00054\n",
      "Epoch:  281, Loss: 0.00056\n",
      "Epoch:  282, Loss: 0.00054\n",
      "Epoch:  283, Loss: 0.00052\n",
      "Epoch:  284, Loss: 0.00056\n",
      "Epoch:  285, Loss: 0.00059\n",
      "Epoch:  286, Loss: 0.00055\n",
      "Epoch:  287, Loss: 0.00051\n",
      "Epoch:  288, Loss: 0.00050\n",
      "Epoch:  289, Loss: 0.00054\n",
      "Epoch:  290, Loss: 0.00063\n",
      "Epoch:  291, Loss: 0.00066\n",
      "Epoch:  292, Loss: 0.00073\n",
      "Epoch:  293, Loss: 0.00060\n",
      "Epoch:  294, Loss: 0.00054\n",
      "Epoch:  295, Loss: 0.00060\n",
      "Epoch:  296, Loss: 0.00060\n",
      "Epoch:  297, Loss: 0.00071\n",
      "Epoch:  298, Loss: 0.00069\n",
      "Epoch:  299, Loss: 0.00060\n",
      "Epoch:  300, Loss: 0.00067\n",
      "Epoch:  301, Loss: 0.00070\n",
      "Epoch:  302, Loss: 0.00072\n",
      "Epoch:  303, Loss: 0.00070\n",
      "Epoch:  304, Loss: 0.00064\n",
      "Epoch:  305, Loss: 0.00061\n",
      "Epoch:  306, Loss: 0.00072\n",
      "Epoch:  307, Loss: 0.00062\n",
      "Epoch:  308, Loss: 0.00054\n",
      "Epoch:  309, Loss: 0.00047\n",
      "Epoch:  310, Loss: 0.00048\n",
      "Epoch:  311, Loss: 0.00050\n",
      "Epoch:  312, Loss: 0.00056\n",
      "Epoch:  313, Loss: 0.00052\n",
      "Epoch:  314, Loss: 0.00050\n",
      "Epoch:  315, Loss: 0.00057\n",
      "Epoch:  316, Loss: 0.00056\n",
      "Epoch:  317, Loss: 0.00048\n",
      "Epoch:  318, Loss: 0.00059\n",
      "Epoch:  319, Loss: 0.00063\n",
      "Epoch:  320, Loss: 0.00061\n",
      "Epoch:  321, Loss: 0.00052\n",
      "Epoch:  322, Loss: 0.00048\n",
      "Epoch:  323, Loss: 0.00045\n",
      "Epoch:  324, Loss: 0.00042\n",
      "Epoch:  325, Loss: 0.00047\n",
      "Epoch:  326, Loss: 0.00044\n",
      "Epoch:  327, Loss: 0.00058\n",
      "Epoch:  328, Loss: 0.00060\n",
      "Epoch:  329, Loss: 0.00056\n",
      "Epoch:  330, Loss: 0.00057\n",
      "Epoch:  331, Loss: 0.00064\n",
      "Epoch:  332, Loss: 0.00063\n",
      "Epoch:  333, Loss: 0.00066\n",
      "Epoch:  334, Loss: 0.00057\n",
      "Epoch:  335, Loss: 0.00066\n",
      "Epoch:  336, Loss: 0.00064\n",
      "Epoch:  337, Loss: 0.00063\n",
      "Epoch:  338, Loss: 0.00058\n",
      "Epoch:  339, Loss: 0.00065\n",
      "Epoch:  340, Loss: 0.00069\n",
      "Epoch:  341, Loss: 0.00064\n",
      "Epoch:  342, Loss: 0.00065\n",
      "Epoch:  343, Loss: 0.00065\n",
      "Epoch:  344, Loss: 0.00065\n",
      "Epoch:  345, Loss: 0.00059\n",
      "Epoch:  346, Loss: 0.00058\n",
      "Epoch:  347, Loss: 0.00055\n",
      "Epoch:  348, Loss: 0.00051\n",
      "Epoch:  349, Loss: 0.00043\n",
      "Epoch:  350, Loss: 0.00044\n",
      "Epoch:  351, Loss: 0.00049\n",
      "Epoch:  352, Loss: 0.00048\n",
      "Epoch:  353, Loss: 0.00042\n",
      "Epoch:  354, Loss: 0.00041\n",
      "Epoch:  355, Loss: 0.00038\n",
      "Epoch:  356, Loss: 0.00045\n",
      "Epoch:  357, Loss: 0.00047\n",
      "Epoch:  358, Loss: 0.00044\n",
      "Epoch:  359, Loss: 0.00046\n",
      "Epoch:  360, Loss: 0.00047\n",
      "Epoch:  361, Loss: 0.00044\n",
      "Epoch:  362, Loss: 0.00041\n",
      "Epoch:  363, Loss: 0.00043\n",
      "Epoch:  364, Loss: 0.00048\n",
      "Epoch:  365, Loss: 0.00048\n",
      "Epoch:  366, Loss: 0.00061\n",
      "Epoch:  367, Loss: 0.00058\n",
      "Epoch:  368, Loss: 0.00055\n",
      "Epoch:  369, Loss: 0.00067\n",
      "Epoch:  370, Loss: 0.00061\n",
      "Epoch:  371, Loss: 0.00058\n",
      "Epoch:  372, Loss: 0.00056\n",
      "Epoch:  373, Loss: 0.00061\n",
      "Epoch:  374, Loss: 0.00059\n",
      "Epoch:  375, Loss: 0.00067\n",
      "Epoch:  376, Loss: 0.00070\n",
      "Epoch:  377, Loss: 0.00060\n",
      "Epoch:  378, Loss: 0.00069\n",
      "Epoch:  379, Loss: 0.00064\n",
      "Epoch:  380, Loss: 0.00053\n",
      "Epoch:  381, Loss: 0.00047\n",
      "Epoch:  382, Loss: 0.00042\n",
      "Epoch:  383, Loss: 0.00043\n",
      "Epoch:  384, Loss: 0.00044\n",
      "Epoch:  385, Loss: 0.00036\n",
      "Epoch:  386, Loss: 0.00039\n",
      "Epoch:  387, Loss: 0.00041\n",
      "Epoch:  388, Loss: 0.00049\n",
      "Epoch:  389, Loss: 0.00047\n",
      "Epoch:  390, Loss: 0.00047\n",
      "Epoch:  391, Loss: 0.00044\n",
      "Epoch:  392, Loss: 0.00052\n",
      "Epoch:  393, Loss: 0.00053\n",
      "Epoch:  394, Loss: 0.00053\n",
      "Epoch:  395, Loss: 0.00052\n",
      "Epoch:  396, Loss: 0.00052\n",
      "Epoch:  397, Loss: 0.00049\n",
      "Epoch:  398, Loss: 0.00045\n",
      "Epoch:  399, Loss: 0.00040\n",
      "Epoch:  400, Loss: 0.00048\n",
      "Epoch:  401, Loss: 0.00056\n",
      "Epoch:  402, Loss: 0.00057\n",
      "Epoch:  403, Loss: 0.00059\n",
      "Epoch:  404, Loss: 0.00075\n",
      "Epoch:  405, Loss: 0.00070\n",
      "Epoch:  406, Loss: 0.00068\n",
      "Epoch:  407, Loss: 0.00059\n",
      "Epoch:  408, Loss: 0.00067\n",
      "Epoch:  409, Loss: 0.00053\n",
      "Epoch:  410, Loss: 0.00049\n",
      "Epoch:  411, Loss: 0.00049\n",
      "Epoch:  412, Loss: 0.00045\n",
      "Epoch:  413, Loss: 0.00044\n",
      "Epoch:  414, Loss: 0.00046\n",
      "Epoch:  415, Loss: 0.00046\n",
      "Epoch:  416, Loss: 0.00050\n",
      "Epoch:  417, Loss: 0.00040\n",
      "Epoch:  418, Loss: 0.00045\n",
      "Epoch:  419, Loss: 0.00044\n",
      "Epoch:  420, Loss: 0.00042\n",
      "Epoch:  421, Loss: 0.00036\n",
      "Epoch:  422, Loss: 0.00035\n",
      "Epoch:  423, Loss: 0.00035\n",
      "Epoch:  424, Loss: 0.00040\n",
      "Epoch:  425, Loss: 0.00040\n",
      "Epoch:  426, Loss: 0.00038\n",
      "Epoch:  427, Loss: 0.00041\n",
      "Epoch:  428, Loss: 0.00046\n",
      "Epoch:  429, Loss: 0.00035\n",
      "Epoch:  430, Loss: 0.00037\n",
      "Epoch:  431, Loss: 0.00048\n",
      "Epoch:  432, Loss: 0.00056\n",
      "Epoch:  433, Loss: 0.00049\n",
      "Epoch:  434, Loss: 0.00048\n",
      "Epoch:  435, Loss: 0.00052\n",
      "Epoch:  436, Loss: 0.00049\n",
      "Epoch:  437, Loss: 0.00046\n",
      "Epoch:  438, Loss: 0.00044\n",
      "Epoch:  439, Loss: 0.00041\n",
      "Epoch:  440, Loss: 0.00044\n",
      "Epoch:  441, Loss: 0.00049\n",
      "Epoch:  442, Loss: 0.00052\n",
      "Epoch:  443, Loss: 0.00057\n",
      "Epoch:  444, Loss: 0.00052\n",
      "Epoch:  445, Loss: 0.00050\n",
      "Epoch:  446, Loss: 0.00057\n",
      "Epoch:  447, Loss: 0.00068\n",
      "Epoch:  448, Loss: 0.00062\n",
      "Epoch:  449, Loss: 0.00062\n",
      "Epoch:  450, Loss: 0.00061\n",
      "Epoch:  451, Loss: 0.00070\n",
      "Epoch:  452, Loss: 0.00080\n",
      "Epoch:  453, Loss: 0.00076\n",
      "Epoch:  454, Loss: 0.00065\n",
      "Epoch:  455, Loss: 0.00075\n",
      "Epoch:  456, Loss: 0.00067\n",
      "Epoch:  457, Loss: 0.00056\n",
      "Epoch:  458, Loss: 0.00057\n",
      "Epoch:  459, Loss: 0.00052\n",
      "Epoch:  460, Loss: 0.00059\n",
      "Epoch:  461, Loss: 0.00056\n",
      "Epoch:  462, Loss: 0.00061\n",
      "Epoch:  463, Loss: 0.00052\n",
      "Epoch:  464, Loss: 0.00057\n",
      "Epoch:  465, Loss: 0.00054\n",
      "Epoch:  466, Loss: 0.00045\n",
      "Epoch:  467, Loss: 0.00042\n",
      "Epoch:  468, Loss: 0.00043\n",
      "Epoch:  469, Loss: 0.00039\n",
      "Epoch:  470, Loss: 0.00040\n",
      "Epoch:  471, Loss: 0.00041\n",
      "Epoch:  472, Loss: 0.00042\n",
      "Epoch:  473, Loss: 0.00056\n",
      "Epoch:  474, Loss: 0.00046\n",
      "Epoch:  475, Loss: 0.00046\n",
      "Epoch:  476, Loss: 0.00045\n",
      "Epoch:  477, Loss: 0.00048\n",
      "Epoch:  478, Loss: 0.00051\n",
      "Epoch:  479, Loss: 0.00048\n",
      "Epoch:  480, Loss: 0.00048\n",
      "Epoch:  481, Loss: 0.00050\n",
      "Epoch:  482, Loss: 0.00044\n",
      "Epoch:  483, Loss: 0.00040\n",
      "Epoch:  484, Loss: 0.00044\n",
      "Epoch:  485, Loss: 0.00042\n",
      "Epoch:  486, Loss: 0.00042\n",
      "Epoch:  487, Loss: 0.00046\n",
      "Epoch:  488, Loss: 0.00041\n",
      "Epoch:  489, Loss: 0.00036\n",
      "Epoch:  490, Loss: 0.00037\n",
      "Epoch:  491, Loss: 0.00040\n",
      "Epoch:  492, Loss: 0.00038\n",
      "Epoch:  493, Loss: 0.00038\n",
      "Epoch:  494, Loss: 0.00034\n",
      "Epoch:  495, Loss: 0.00043\n",
      "Epoch:  496, Loss: 0.00042\n",
      "Epoch:  497, Loss: 0.00040\n",
      "Epoch:  498, Loss: 0.00040\n",
      "Epoch:  499, Loss: 0.00039\n"
     ]
    }
   ],
   "source": [
    "for k in range(epochs):\n",
    "    preds=[]\n",
    "    labels=[]\n",
    "    for j in range(int(len(train_X)/batch_size)):\n",
    "        batch=[]\n",
    "        label=[]\n",
    "        for i in range(batch_size):\n",
    "            batch.append(train_X[j*batch_size+i])\n",
    "            label.append(train_Y[j*batch_size+i])\n",
    "    \n",
    "        batch=torch.tensor(batch).to(torch.float)\n",
    "        label=torch.tensor(label).to(torch.float)\n",
    "        out,pred=model(batch)\n",
    "        loss=loss_func(pred,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds+=pred.tolist()\n",
    "        labels+=label.tolist()\n",
    "    preds=torch.tensor(preds).to(torch.float)\n",
    "    labels=torch.tensor(labels).to(torch.float)\n",
    "    loss=loss_func(preds,labels)\n",
    "    print('Epoch: {:4}, Loss: {:.5f}'.format(k, loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 2., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test='ionic'\n",
    "test=list(test)\n",
    "test_batch=[[0 for i in range(26)]]\n",
    "for i in test:\n",
    "    test_batch[0][letter2idx[i]]+=1\n",
    "test_batch=torch.tensor(test_batch).to(torch.float)\n",
    "print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1667e-06, 2.0177e-02, 1.3588e-01, 2.4982e-01, 2.5273e-01, 2.2360e-01,\n",
      "         1.1779e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(4.8770, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out,pred=model(test_batch)\n",
    "print(pred)\n",
    "mean=0\n",
    "cnt=1\n",
    "for weight in pred[0]:\n",
    "    mean+=weight*cnt\n",
    "    cnt+=1\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manly', 'molar', 'havoc', 'impel', 'condo', 'judge', 'extra', 'poise', 'aorta', 'excel', 'lunar', 'third', 'slate', 'taper', 'chord', 'probe', 'rival', 'usual', 'spoke', 'apply', 'naive', 'knock', 'braid', 'infer', 'joust', 'amber', 'woken', 'adore', 'torso', 'chafe', 'eject', 'study', 'undue', 'tepid', 'happy', 'clen', 'itchy', 'feast', 'drive', 'prime', 'axiom', 'brave', 'avert', 'glyph', 'there', 'baker', 'snarl', 'maple', 'inane', 'valet', 'medal', 'unite', 'rainy', 'spell', 'begin', 'stale', 'dream', 'photo', 'aloud', 'inept']\n",
      "tensor([[0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 2., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "tensor([[0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 2., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 1., 2., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1.]])\n",
      "tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 2., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "data_chi = data_frame.loc[1:60 ,\"Unnamed: 3\"].tolist()\n",
    "prediction={}\n",
    "print(data_chi)\n",
    "for word in data_chi:\n",
    "    test=word\n",
    "    test=list(test)\n",
    "    test_batch=[[0 for i in range(26)]]\n",
    "    for i in test:\n",
    "        test_batch[0][letter2idx[i]]+=1\n",
    "    test_batch=torch.tensor(test_batch).to(torch.float)\n",
    "    out,pred=model(test_batch)\n",
    "    prediction[word]=pred[0].tolist()\n",
    "    print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'manly': [0.0009690403821878135, 0.030068805441260338, 0.17544540762901306, 0.3502339720726013, 0.28928643465042114, 0.13368792831897736, 0.020308438688516617], 'molar': [0.004211779683828354, 0.06830929219722748, 0.2673470973968506, 0.34428614377975464, 0.21597768366336823, 0.08480572700500488, 0.015062268823385239], 'havoc': [0.001061141025274992, 0.017458807677030563, 0.09508875757455826, 0.24099870026111603, 0.31522461771965027, 0.2518423795700073, 0.07832560688257217], 'impel': [0.004535948392003775, 0.06155858188867569, 0.2478121966123581, 0.3473207354545593, 0.2285117208957672, 0.09384476393461227, 0.016416020691394806], 'condo': [0.0017141696298494935, 0.02231675758957863, 0.16596682369709015, 0.317193865776062, 0.2947966754436493, 0.16597266495227814, 0.032039087265729904], 'judge': [0.00029112905031070113, 0.02171826735138893, 0.11826103925704956, 0.31340786814689636, 0.33166515827178955, 0.18289262056350708, 0.031763955950737], 'extra': [0.0004081875376868993, 0.07360334694385529, 0.2196250706911087, 0.30720892548561096, 0.22844117879867554, 0.12525822222232819, 0.04545506462454796], 'poise': [7.74079016991891e-05, 0.08064060658216476, 0.32154205441474915, 0.3641252815723419, 0.17572711408138275, 0.05258937552571297, 0.005298158153891563], 'aorta': [0.006470073945820332, 0.08217233419418335, 0.2890748381614685, 0.33916595578193665, 0.19898317754268646, 0.07521649450063705, 0.008917035534977913], 'excel': [0.00020293980196584016, 0.011653324589133263, 0.10141998529434204, 0.3280656635761261, 0.37892118096351624, 0.1623501479625702, 0.01738666743040085], 'lunar': [0.0005640293238684535, 0.06996655464172363, 0.32091665267944336, 0.3899790644645691, 0.17600098252296448, 0.03994693607091904, 0.0026258996222168207], 'third': [0.004868307150900364, 0.12005793303251266, 0.5066348910331726, 0.2706974148750305, 0.08122953027486801, 0.015897098928689957, 0.0006148980464786291], 'slate': [0.02331147901713848, 0.13580134510993958, 0.3108668625354767, 0.29717567563056946, 0.1562558114528656, 0.059404656291007996, 0.017184196040034294], 'taper': [0.002120408695191145, 0.07700025290250778, 0.24968652427196503, 0.3176499903202057, 0.21561020612716675, 0.10651624947786331, 0.0314164012670517], 'chord': [0.0014950670301914215, 0.0645170584321022, 0.408053457736969, 0.3504965305328369, 0.14262054860591888, 0.03178262710571289, 0.0010347525821998715], 'probe': [4.597776205628179e-05, 0.06542299687862396, 0.22726136445999146, 0.3215506970882416, 0.2301875203847885, 0.11820371448993683, 0.03732772916555405], 'rival': [0.004617057275027037, 0.06948622316122055, 0.252603143453598, 0.34048888087272644, 0.2228809893131256, 0.09262394905090332, 0.017299821600317955], 'usual': [0.00022484053624793887, 0.029516179114580154, 0.18256759643554688, 0.3856787383556366, 0.2899167835712433, 0.1016339585185051, 0.010461829602718353], 'spoke': [1.212542974826647e-05, 0.0638623908162117, 0.29763931035995483, 0.3822653293609619, 0.1953524947166443, 0.056626852601766586, 0.004241436254233122], 'apply': [0.00026529666502028704, 0.059214089065790176, 0.2917184829711914, 0.37284427881240845, 0.2016608864068985, 0.06659248471260071, 0.007704477291554213], 'naive': [0.0015009990893304348, 0.06736143678426743, 0.22550168633460999, 0.3535001277923584, 0.24012960493564606, 0.09919653087854385, 0.012809616513550282], 'knock': [1.9521874492056668e-05, 0.018176842480897903, 0.18919067084789276, 0.4211549162864685, 0.2833144962787628, 0.08397450298070908, 0.004169180989265442], 'braid': [0.0016076089814305305, 0.09907293319702148, 0.372361421585083, 0.34418976306915283, 0.14931674301624298, 0.031041305512189865, 0.002410252345725894], 'infer': [1.5293717581243982e-07, 0.04225527495145798, 0.14902178943157196, 0.33284130692481995, 0.3015959858894348, 0.15533146262168884, 0.0189539585262537], 'joust': [0.0022251869086176157, 0.06277641654014587, 0.2639152705669403, 0.36009806394577026, 0.22186128795146942, 0.07694736123085022, 0.012176379561424255], 'amber': [0.009548445232212543, 0.07573441416025162, 0.2263091802597046, 0.30180075764656067, 0.22863814234733582, 0.12382921576499939, 0.034139782190322876], 'woken': [1.912492280098377e-06, 0.008846544660627842, 0.08436816930770874, 0.30142322182655334, 0.37044012546539307, 0.21336685121059418, 0.021553145721554756], 'adore': [0.027208687737584114, 0.1842535436153412, 0.31019291281700134, 0.2767261862754822, 0.14715327322483063, 0.049974940717220306, 0.004490499384701252], 'torso': [0.0016505576204508543, 0.04669307917356491, 0.3576832711696625, 0.358066588640213, 0.17268098890781403, 0.05840034782886505, 0.004825067240744829], 'chafe': [0.002829582430422306, 0.07902487367391586, 0.2769552171230316, 0.3453746438026428, 0.20580559968948364, 0.07745733112096786, 0.012552741914987564], 'eject': [5.0337883294560015e-05, 0.011032222770154476, 0.08417296409606934, 0.3144003748893738, 0.38229620456695557, 0.1908789873123169, 0.017168860882520676], 'study': [0.007587924599647522, 0.07845769077539444, 0.2659543752670288, 0.3274436593055725, 0.20847122371196747, 0.09090572595596313, 0.02117936499416828], 'undue': [0.00014701747568324208, 0.03221004828810692, 0.20291578769683838, 0.3492181599140167, 0.2778277099132538, 0.12343686074018478, 0.014244494959712029], 'tepid': [1.7364796804031357e-05, 0.07864298671483994, 0.3352687358856201, 0.37370288372039795, 0.16560637950897217, 0.0434638112783432, 0.003297829534858465], 'happy': [2.974124981847126e-05, 0.0552503764629364, 0.30893266201019287, 0.3919515907764435, 0.18856966495513916, 0.051394179463386536, 0.00387177593074739], 'clen': [0.018516721203923225, 0.1211320012807846, 0.3138665556907654, 0.3144189715385437, 0.16530537605285645, 0.05796350538730621, 0.008796815760433674], 'itchy': [0.0004965583793818951, 0.060512661933898926, 0.2824665904045105, 0.3645247220993042, 0.2079438418149948, 0.07413242012262344, 0.009923163801431656], 'feast': [0.013654666021466255, 0.09673108160495758, 0.2545444965362549, 0.30284494161605835, 0.20504650473594666, 0.09790103137493134, 0.029277309775352478], 'drive': [0.01712842658162117, 0.10529083758592606, 0.28847643733024597, 0.31168103218078613, 0.18183092772960663, 0.07868203520774841, 0.016910359263420105], 'prime': [0.003535038558766246, 0.07890588790178299, 0.26249533891677856, 0.3254677355289459, 0.2100154310464859, 0.09576448053121567, 0.023816123604774475], 'axiom': [0.001826108549721539, 0.04235171899199486, 0.1921471804380417, 0.34233370423316956, 0.27052441239356995, 0.12639127671718597, 0.024425579234957695], 'brave': [0.0006889650248922408, 0.04400571063160896, 0.16967271268367767, 0.2996369004249573, 0.26674437522888184, 0.16031789779663086, 0.05893339589238167], 'avert': [0.008224671706557274, 0.07082126289606094, 0.24169115722179413, 0.3184240162372589, 0.2226565033197403, 0.10934583842754364, 0.0288365688174963], 'glyph': [5.8160516346106306e-05, 0.02201835997402668, 0.2289269119501114, 0.4785080552101135, 0.21738208830356598, 0.05089765414595604, 0.0022088258992880583], 'there': [0.004307886585593224, 0.1237509697675705, 0.3311136066913605, 0.34613388776779175, 0.15365050733089447, 0.036098413169384, 0.004944739863276482], 'baker': [0.00011398684728192165, 0.042095378041267395, 0.1347823590040207, 0.24932102859020233, 0.2596178948879242, 0.19688062369823456, 0.1171887218952179], 'snarl': [0.004111034795641899, 0.05179224908351898, 0.19223013520240784, 0.30138707160949707, 0.2763842046260834, 0.14176510274410248, 0.032330118119716644], 'maple': [0.0034798108972609043, 0.05347158759832382, 0.23740608990192413, 0.3572547435760498, 0.2395366132259369, 0.09466338902711868, 0.014187706634402275], 'inane': [0.0012967982329428196, 0.10023454576730728, 0.24906198680400848, 0.29406794905662537, 0.20333412289619446, 0.12129011005163193, 0.030714472755789757], 'valet': [0.002471699845045805, 0.05109737068414688, 0.20912203192710876, 0.34228822588920593, 0.2574694752693176, 0.11684473603963852, 0.020706482231616974], 'medal': [0.0023516991641372442, 0.052033182233572006, 0.23556047677993774, 0.3710567057132721, 0.24460481107234955, 0.08603239804506302, 0.008360727690160275], 'unite': [0.00201409007422626, 0.1133304014801979, 0.3074319064617157, 0.34711694717407227, 0.17670929431915283, 0.05007071793079376, 0.0033267668914049864], 'rainy': [0.0013833482516929507, 0.18351396918296814, 0.3515447974205017, 0.29588794708251953, 0.12513698637485504, 0.038042206317186356, 0.004490718711167574], 'spell': [0.0007130593294277787, 0.03423047065734863, 0.20669889450073242, 0.3635076880455017, 0.26519539952278137, 0.11273471266031265, 0.016919823363423347], 'begin': [0.0012937062419950962, 0.04830874502658844, 0.22394607961177826, 0.36349838972091675, 0.25086307525634766, 0.09752578288316727, 0.014564269222319126], 'stale': [0.02331147901713848, 0.13580134510993958, 0.3108668625354767, 0.29717567563056946, 0.1562558114528656, 0.059404656291007996, 0.017184196040034294], 'dream': [0.038820039480924606, 0.14742924273014069, 0.30873212218284607, 0.301586776971817, 0.15512296557426453, 0.04505099728703499, 0.0032578676473349333], 'photo': [0.00014892251056153327, 0.05576712638139725, 0.33661291003227234, 0.4234122037887573, 0.15894348919391632, 0.024911707267165184, 0.0002036641089944169], 'aloud': [0.009918349795043468, 0.17481663823127747, 0.3074195086956024, 0.29021984338760376, 0.15759946405887604, 0.055189475417137146, 0.0048367795534431934], 'inept': [1.8919596186606213e-05, 0.04855590686202049, 0.26266345381736755, 0.3952478766441345, 0.22599832713603973, 0.06477972865104675, 0.002735721878707409]}\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdec6ca5ce638a4776cd655f71204534c127e30145c88be198b9bebdb38fc0db"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 ('snow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
